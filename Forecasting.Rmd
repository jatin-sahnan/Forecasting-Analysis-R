---
title: '**\textcolor{blue}{Forecasting Analytics}**'
author: '**\textcolor{blue}{Jatin Sahnan}**'
output: pdf_document
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(comment = NA)
```



```{r include=FALSE}
##Loading Required Libraries
library(readxl)
library(fpp2)
library(tidyverse)
library(forecast)
library(TSstudio)
library(tseries)
library(Metrics)
```


## **Consider  the  data  set  SouvenirSales.xls  (1995Jan - 2001Dec) that  gives  the monthly  sales  of  souvenir  at  a  shop  in  New York. Back in 2001, an analyst was appointed to forecast sales for the next 12 months (Year2002). The analyst portioned the data by keeping the last 12 months of  data  (year2001)  as validation set, and the remaining data as training set.**


### **Plot the time series of the original data.**

```{r echo=c(2,3)}
## importing the excel file
df<-read_excel("SouvenirSales.xlsx")
head(df)
```
```{r echo=c(2,3)}
## Converting the data to time series
sales.ts<-ts(df$Sales, start = c(1995,1), frequency = 12)
summary(sales.ts)
```
```{r echo=c(2)}
## Plotting time series for Souvenir Sales data
plot.ts(sales.ts/1000, ylab = "Sales (in thousands)", xlab="Year" , main = "Souvenir sales time series plot", bty = "l")
```

### \textcolor{red}{From the above plot it is clearly visible that there is an upward trend present and also, since the values are increasing multiplicative sesonality is present. It is clearly evident that in every year there is a peak at the end of year.}

***

### **Fitting a linear trend model with additive seasonality (Model A) and exponential trend model with multiplicative seasonality (Model B).  Consider  January  as  the  reference  group  for  each model.**

```{r echo=TRUE}
## Splitting the data into train and validation data
train <- window(sales.ts,end=c(2000,12), frequency=12)
valid <- window(sales.ts,start=c(2001,1), frequency=12)
train
```
## **\textcolor{red}{MODEL A}**

```{r echo=TRUE}
## Regression Coefficients for Linear trend model with additive seasonality
ModelA<-tslm(train ~ trend + season)
summary(ModelA)
```
```{r echo=TRUE}
## Forecasting for Model A on validation set
A_fore<-forecast(ModelA , h=length(valid), level = 0)
summary(A_fore)
```


## **\textcolor{red}{MODEL B}**

```{r echo=TRUE}
## Regression Coefficients for Exponential trend model with multiplicative seasonality
ModelB<-tslm(train ~ trend + season, lambda = 0)
summary(ModelB)
```
```{r echo=TRUE}
## Forecasting for Model B on validation set
B_fore<-forecast(ModelB , h=length(valid), level = 0)
summary(B_fore)
```

***

### **Selecting   best   model   considering   RMSE   as   the metric**

## **\textcolor{red}{Line Plot showing Forecasts from both models with actual data}**

```{r echo=c(2:6)}
## Plotting line plot for forecasts from both Model A & B
plot(B_fore,lwd=1.5, xlab ="Year", ylab= "Sales", main="Forecasted Sales from both models")
lines(A_fore$fitted,col="red",lwd=2, lty="dotted")
lines(B_fore$fitted,col="blue",lwd=2, lty=1)
lines(valid, lwd=1.5)
legend(x="topleft", legend=c("Actual", "Model A","Model B"),col=c("black", "red", "blue"), lty=c(1,3,1), lwd=2)
```

## **\textcolor{red}{Validation Set Residuals from both Models}**

```{r echo=TRUE}
plot(A_fore$residuals, main= "Residual Plot for Model A using Validation Set", ylab="Residual", lwd=1.5)
```

```{r echo=TRUE}
plot(B_fore$residuals, main= "Residual Plot for Model B using Validation Set", ylab="Residual", lwd=1.5)
```

## **\textcolor{red}{RMSE for both Models}**
```{r echo=TRUE}
A_rmse<-rmse(valid,A_fore$mean)
cat("The RMSE for model A is:", A_rmse)
```

```{r echo=TRUE}
B_rmse<-rmse(valid,B_fore$mean)
cat("The RMSE for model B is:", B_rmse)
```

### \textcolor{red}{As we can clearly see, RMSE for model B (7101.44) is much less than RMSE for model A (17451.55), and thus, Model B is a better fit considering RMSE as a metric. We can understand the same thing from the Forecasts line plot, as the plot line for model B is more closer to actual plot line and thus, we can say that model B is a better forecasting model.}

***

### **Examining the additive model**

```{r echo=TRUE}
summary(ModelA)
```
### \textcolor{red}{On examining the additive model, we observe that December has the highest average sales during the year. The estimated trend coefficient in Model A above indicates that during the year the sales increase every month by 245.36 units.} 
 
***

### **Examining the multiplicative model**

```{r echo=TRUE}
summary(ModelB)
```

### \textcolor{red}{On examining the multiplicative model, the coefficient of October implies that sales in October is 72.95 percent increase over the sales in January. The estimated trend coefficient in Model B above indicates that during the year the sales increase every month by 2.11 percent.}

***

### **Using the best model type to forecast the sales in January 2002.**

### \textcolor{red}{Model B was the best model and hence, we apply the multiplicative model on complete time series data for forecasting Januray 2002 sales.}

```{r echo=TRUE}
## Using the multiplicative model on complete time series data
final_model <- tslm(sales.ts ~ trend + season, lambda=0)
summary(final_model)
```
```{r echo=TRUE}
##Forecasting January 2002 sales
jan_fore <- forecast(final_model, h=1, level=0)
jan_fore

```

### \textcolor{red}{The forecasted sales for January 2002 is 13484.06 units.}

***

### \textcolor{red}{For plotting ACF and PACF plots we use the residuals of training set for Model B.}

```{r echo=TRUE}
## Plotting ACF plot
acf(ModelB$residuals, lag.max = 20, main= "ACF plot for Model B Residuals")
```
```{r echo=TRUE}
## Plotting PACF plot
pacf(ModelB$residuals, lag.max = 20, main= "PACF plot for Model B Residuals")
```

### \textcolor{red}{If we look at the ACF plot (ignoring the Lag 0), we see that the second Lag has has higher correlation than that at the first Lag, which is not usually not possible and hence, we migh not consider these as significant.}

### \textcolor{red}{To chose the best AR[p] model we look at PACF plot and we see that 2 significant correlations at Lag 1 and Lag 2, thus the order of AR[p] model is two, i.e. p=2 and hence, we will implement AR[2] model.}

***

### \textcolor{red}{As concluded above, we will be fitting AR[2] model to the training set residuals.}

```{r echo=TRUE}
B_ar <- arima(ModelB$residuals, order=c(2,0,0))
summary(B_ar)
```
### \textcolor{red}{As we can see above, the regression coefficients lie between -2 and 2 and alse the RMSE is 0.1402, that means that model is a good fit and we can conclude, that our intuition in part(g) for selecting AR[2] model was correct.}

***

### **Using the best regression model and AR(p)model,forecasting the sales in January2002.**

### \textcolor{red}{We will use the final model , and apply AR[2] model on the final model residuals. We have used the entire dataset for fitting the model.}

```{r echo=TRUE}
## Applying AR[2] model
sales_ar <- arima(final_model$residuals, order=c(2,0,0))

## Forecasting the sales for January 2002
jan_fore2 <- forecast(sales_ar, h=1, level=0)
jan_fore_final <- jan_fore2$mean+jan_fore$mean
jan_fore_final
```

### \textcolor{red}{The forecasted sales for January 2002 is 13484.13 units.}
